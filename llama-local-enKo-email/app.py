import streamlit as st
from langchain.prompts import PromptTemplate
# from langchain.llms import CTransformers
# C TransformersëŠ” Llama, GPT4All-J, MPT, Falconê³¼ ê°™ì€ ë‹¤ì–‘í•œ ì˜¤í”ˆ ì†ŒìŠ¤ ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤.
from langchain_community.llms.ctransformers import CTransformers
# ollama llama3.1model ì—°ê²°í•˜ê¸°
from langchain_ollama.llms import OllamaLLM


def getLLMResponse(form_input, email_sender, email_recipient):
    """
    getLLMResponse í•¨ìˆ˜ëŠ” ì£¼ì–´ì§„ ì…ë ¥ì„ ì‚¬ìš©í•˜ì—¬ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ìœ¼ë¡œë¶€í„° ì´ë©”ì¼ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.

    ë§¤ê°œë³€ìˆ˜:
    - form_input: ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì´ë©”ì¼ ì£¼ì œ.
    - email_sender: ì´ë©”ì¼ì„ ë³´ë‚¸ ì‚¬ëŒì˜ ì´ë¦„.
    - email_recipient: ì´ë©”ì¼ì„ ë°›ëŠ” ì‚¬ëŒì˜ ì´ë¦„.

    ë°˜í™˜ê°’:
    - LLMì´ ìƒì„±í•œ ì´ë©”ì¼ ì‘ë‹µ í…ìŠ¤íŠ¸.
    """

    # Llama-2-7B-Chatìš© ë˜í¼: CPUì—ì„œ Llama 2 ì‹¤í–‰

    # í€€íƒ€ì´ì œì´ì…˜ì€ ëª¨ë¸ì˜ ì •ë°€ë„ë¥¼ 16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì ì—ì„œ 8ë¹„íŠ¸ ì •ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì˜ í¬ê¸°ë¥¼ ì¤„ì´ê³  ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ ìì›ì´ ì œí•œëœ ì¥ì¹˜ì—ì„œë„ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

    # C Transformers is the Python library that provides bindings for transformer models implemented in C/C++ using the GGML library
    # ë§Œì•½ ì»´í“¨í„° ì‚¬ì–‘ì— ë§ì¶”ì–´ì„œ ëª¨ë¸ì„ local ì»´í“¨í„°ì— ë‹¤ìš´ë°›ê³  model="ë‹¤ìš´ë°›ì€ ëª¨ë¸ëª…"ì„ ì‘ì„±í•´ì„œ ì§„í–‰í•˜ì‹œë©´ ë©ë‹ˆë‹¤. ìš©ëŸ‰ì´ ì‘ì„ìˆ˜ë¡ ê²½ëŸ‰í™” ëœ ë²„ì „ì´ì—¬ì„œ ì„±ëŠ¥ì´ ì¡°ê¸ˆ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    # GPTì™€ ê°™ì€ ì–¸ì–´ ëª¨ë¸ì—ì„œëŠ” ì´ëŸ¬í•œ íŒŒì¼ í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ í•™ìŠµëœ ê°€ì¤‘ì¹˜ì™€ êµ¬ì¡°ë¥¼ ì €ì¥í•˜ê³ , ì´ë¥¼ ì¶”ë¡ (inference) ì‹œ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ì´ëŸ¬í•œ íŒŒì¼ í˜•ì‹ì€ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì €ì¥í•˜ê³  ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆê²Œ í•˜ì—¬, ë‹¤ì–‘í•œ í”Œë«í¼ê³¼ í™˜ê²½ì—ì„œ ëª¨ë¸ ì¶”ë¡ ì„ ì›í™œí•˜ê²Œ í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.
    # ë”°ë¼ì„œ, GGUF ë° GGML íŒŒì¼ í˜•ì‹ì€ GPTì™€ ê°™ì€ ì–¸ì–´ ëª¨ë¸ì˜ ë§¥ë½ì—ì„œ ëª¨ë¸ ì¶”ë¡ ì„ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì¤‘ìš”í•œ íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤

    # llm = CTransformers(model='./llama-2-7b-chat.ggmlv3.q8_0.bin',  # https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/tree/main
    #                     model_type='llama',
    #                     config={'max_new_tokens': 512,
    #                             'temperature': 0.01})

    # ollama llama3.1 ë¶€ë¶„ ì—°ê²°
    llm = OllamaLLM(model="llama3.1:8b", temperature=0.7)

    template = """ 
    {email_topic} ì£¼ì œë¥¼ í¬í•¨í•œ ì´ë©”ì¼ì„ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n\në³´ë‚¸ ì‚¬ëŒ: {sender}\në°›ëŠ” ì‚¬ëŒ: {recipient} ì „ë¶€ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì„œ ì•Œë ¤ì£¼ì„¸ìš”.
    \n\nì´ë©”ì¼ ë‚´ìš©:
    """

    # ìµœì¢… PROMPT ìƒì„±
    prompt = PromptTemplate(
        input_variables=["email_topic", "sender", "recipient"],
        template=template,
    )

    # LLMì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
    # ì§€ë‚œ ì£¼ì— langchainì€ ì•„ë˜ì˜ 'invoke' í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ê²ƒì„ ê¶Œì¥í–ˆìŠµë‹ˆë‹¤ :)
    response = llm.invoke(prompt.format(email_topic=form_input,
                                        sender=email_sender, recipient=email_recipient,))
    print(response)

    return response


st.set_page_config(
    page_title="ì´ë©”ì¼ ìƒì„±ê¸° ğŸ“®",
    page_icon='ğŸ“®',
    layout='centered',
    initial_sidebar_state='collapsed'
)
st.header("ì´ë©”ì¼ ìƒì„±ê¸° ğŸ“® ")

form_input = st.text_area('ì´ë©”ì¼ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”', height=275)

# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ê¸° ìœ„í•œ UI ì—´ ìƒì„±
col1, col2 = st.columns([10, 10])
with col1:
    email_sender = st.text_input('ë³´ë‚¸ ì‚¬ëŒ ì´ë¦„')
with col2:
    email_recipient = st.text_input('ë°›ëŠ” ì‚¬ëŒ ì´ë¦„')

submit = st.button("ìƒì„±í•˜ê¸°")

# 'ìƒì„±í•˜ê¸°' ë²„íŠ¼ì´ í´ë¦­ë˜ë©´, ì•„ë˜ ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
if submit:
    st.write(getLLMResponse(form_input, email_sender,
             email_recipient))
